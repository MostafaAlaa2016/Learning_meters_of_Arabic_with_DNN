\chapter{Model Training}\label{Ch:Model_Training}


In this chapter, we discuss the training and experiment design. This phase starts by exploring the ratio of Training, Testing, and Validation. Choosing the correct percentage of training dataset compared to the Testing and Validation in Deep learning differs from normal machine learning structure, and affects the model performance. One must choose the correct Neural Network and RNN architecture.

\clearpage


In a normal machine learning project, the dataset would be divided into approximately 60\% training, 30\% testing and 10\% validation. However, in The Deep Learning, the amount of data profoundly affects the model performance. 
Hence, the more data fed as training, the more performance results the model can achieve on test data (Assuming regularization and the required generalization were conducted in the model training phase). 
The main reason to change the split size is due to the size of the dataset we previously worked on; for example 1\% of 1M sample is 10k, which is big enough for such experiments. 
However, if we work on a 10k sample, we need around 30\% 3k sample to have confidence in our model. It therefore depends on the problem and size of the dataset.

The current research addresses Poem Comprehensive Dataset (PCD)~\cite{ArabicpoetryDS}. We are interested in the 16 classic meters, attributed to Al-Farahidi. These meters comprise the majority of the dataset with a total number of 1,722,321 verses. Figure~\ref{Fig:Data_Size_Distribution} shows an ordered bar chart based on the number of verses per meter. We trained all our models based on 80\%, around 1,377,856 verses. Our testing data (development) is 10\%, around 172,232 verses, and our Validation data around 172,232 verses.

We can show the training phase designed as a data representation configuration and an RNN configuration. The number of experiments is the cross product of both data representations and RNN configuration; for example, If we have 12 data representations and 12 RNN configurations, the total number of experiments will be 144. It should be emphasised that:

\begin{itemize}
 \item Data representation feature is a general feature applied to the dataset, not a hand-drafted feature. More details on~\nameref{Sec:Data_Rep_Param}.
 \item Data representation feature is affected by Arabic language pronunciation, and some features provide more information than others.
 \item RNN configurations are the parameters related to the Network model development training, and are used after many experiments to find and tune the best configurations. This means we did more than the number of experiment written in this research but publish only the best results overall. More details are provided in sec~\nameref{Sec:Rnn_Param}.
 \item The number of verses (344,464) used in testing and validation is significant, confirming that the model was tested on all types of verses and inspiring confidence in the results.

\end{itemize}

\section{Parameters of Data Representation}\label{Sec:Data_Rep_Param}

Arabic language parameters have the following types: Diacritics, Trimming, and Encoding. Every type has its effect on the data and the performance on the model learning rate. Each is explained in detail in the following subsections.

\subsection{Diacritics}

The Arabic dataset contains the verse with diacritics. We can feed the network the characters with and without diacritics. With diacritics, it will be much easier for the network to learn, since they provide more information on the pronunciation. Moreover, they provide more information relating to the vowel and consonant sounds in the letters. However, as we discussed in Data Encoding Chapter~\nameref{Ch:Data_Encoding}, the inclusion or otherwise of diacritics has the same length in input vector size.


\subsection{Trimming Small Classes}

The Arabic poem dataset, as stated in Figure~\ref{Fig:Data_Size_Distribution}, is unbalanced. Hence, as part of our research, we consider the dataset representation as a Full dataset and Trimmed dataset. This allows us to study the effect of this unbalance. We not only explore the impact of the unbalanced dataset, but also apply a technique to solve this issue~\nameref{Sec:W_Loss}. The trimmed classes are five classes which have less than 1\% of the total dataset. We present these classes as all the classes on the left side of the horizontal red line in Figure~\ref{Fig:Data_Size_Distribution}. Therefore, the classes after trimming total 11, and in the Full dataset all 16 meters are presented.

\subsection{Encoding Techniques}

As explained previously, there are three different encoding methods~\nameref{Ch:Data_Encoding}. Although all carry the same information, it was expected that every encoding has its own behaviors (as below):

\begin{itemize}
 \item \textbf{Running Time}: It was expected that the running time would differ from one encoding type to others. This information is important if someone is conducting an experiment with limited time and needs the results as fast as possible.
 \item \textbf{Required Resources}: It was expected that different resources will be required from one encoding type to others. This information is important if someone has an experiment with limited resources.
 
 \item \textbf{Learning Rate}: Some encoding will learn faster than others (Note: Learning Rate not the overall performance) for example, one encoding can achieve 80\% on training performance after four epoch, but another one can reach the same percentage after 20\% epoch.
 \item \textbf{Overall Performance}: The final performance percentage which can be achieved with each encoding technique. (Note: the best learning rate may be the method which takes most time.) Therefore, the researcher who will use this encoding should decide which one will be used based on the criteria needed.

 \item \textbf{Overall Performance}: The final performance percentage which can be achieved with every encoding technique (Note: the best can be the worst in learning rate or the method which take much time). So, the researcher who will use this encoding should decide which one will be used based on the criteria needed.

\end{itemize}

\subsection{Data Representation Matrix}

The data representation matrix is the cross product of the Diacritics 2, Data Encoding 3, and Trimming 2: total 12 combinations table~\ref{Tab:Data_Representation_Matrix} shows this matrix combination. Example (With diacritic + \textit{one-hot} + Full)


\begin{table}[t]
 \centering
 \begin{tabular}{c c c c c c}
 \hline
 \textbf{\#} & \textbf{Diacritic} & \multicolumn{3}{c}{\textbf{Encoding Types}} & \textbf{Trimming} \\
 
 \hline
 1 & With diacritic & \textit{one-hot} & \textit{two-hot} & \textit{binary} & Full \\
 \hline
 2 & Without diacritic & \textit{one-hot} & \textit{two-hot} & \textit{binary} & Trimmed \\
 \hline
 \end{tabular}
 \caption{Data Representation Combination Matrix}\label{Tab:Data_Representation_Matrix}
\end{table}


% \begin{figure}[H]
% \centering
% \begin{tikzpicture}
%  \input{./Figures/Ch_4_Dataset/dataset_percentage_ar.tex}
% \end{tikzpicture}%
% \caption{Arabic dataset class size (number of verses) ordered descendingly on y-axis vs. corresponding meter name on the x-axis.}\label{Fig:data_percentage_distribution}
% \end{figure}
\section{Parameters of Network Configuration}\label{Sec:Rnn_Param}

% RNN,LSTM,BI_LSTM
As explained previously, Recurrent Neural Networks (RNN) show the ability to solve language model problems in Section~\nameref{Sec:RNN}. We used RNNs with Long Short Term Memory (LSTM)~\nameref{Sec:LSTM} as the main architecture for our experiments. We also used BI-LSTM, discussed in Section~\nameref{Sec:Bi_Lstm}, as an alternative way to test the effect of BI-Directional LSTM, to check the learning patterns with the two directions.

We also thought using BI-LSTM will support the model to learn the \textit{tafa'il} for every class as it can be combined with the sound of music from both perspectives. We will explain our argument and the effects on the results in Section~\nameref{Sec:Results}. In RNNs network configuration parameters, we took 100k sample of the data and applied numerous experiments and trials to find the best network configuration parameters which we show in the next part. However, due to the hardware limitations, we didnâ€™t apply much higher configuration parameters, due to the memory issue. We can mention that, for example, when we were trying to increase the unit size over 82 and the layers more than 7 we had insufficient memory exceptions and sometimes reduced for experiment stability. There are four parameters:
\begin{itemize}
\item \textbf{Cell Type}: We used LSTM and BI-LSTM.
\item \textbf{Layers}: We tried many numbers of layers and we found the optimum number based on our problem to be 4 and 7 layers.
\item \textbf{Cell Unit Size}: We also tried many numbers but the best results were achieved between 50 and 82.
\item \textbf{Weighting Model}: As shown in Figure~\ref{Fig:Data_Size_Distribution}, classes are unbalanced. Hence, as an alternative to removing the small classes, we tried to keep all classes, weighting the loss function to account for the relative class size. We introduced a new weighting function, explained in the next sub-section~\nameref{Sec:W_Loss} to help work on all the dataset. We therefore have two combinations; one with weighting loss and one without weighting loss.
 
 \end{itemize}

The total number of combinations is 16, which is $4$ parameters: each one has $2$ types. Thus, the total will be $2^4=16$; and hence, there are 16 different network configurations to run on each of the 12 data representations above. This results in $16 \times 12 = 192$ different experiments (or models). Hence, there are 96 different network configurations to run on each of the 2 data representations above. This results in $96 \times 2 = 192$ different experiments (or models), whose accuracy is presented on the y-axis of Figure~\ref{Fig:ArabicModelsResults}. For all the 192 experiments, networks are trained using dropout of $0.2$, batch size is of $2048$, with Adam optimizer, and 10\% for each validation and testing sets.

\subsection{Working on Unbalanced data using Weighted Loss}\label{Sec:W_Loss}

Unbalanced dataset issue is one of the important problems we addressed in research. We strove to overcome the unbalanced dataset and to protect our model from this issue. We therefore introduced a Weighting function~\ref{eq:training_weighted_fun} where nc is the sample size of class c, c = 1,2,...C, and C is the total number of classes.%

\begin{equation}\label{eq:training_weighted_fun}
 w_c = \left(\frac{\frac{1}{n_c}}{\sum_{c'} \frac{1}{n_{c'}}} \right)
\end{equation}

The idea was to increase the loss for the small classes, given that the number of verses of small classes is small, so the output will be bigger than the loss in cases where the class has many verses. To have a clear explanation, we can show equation~\ref{eq:training_weighted_fun_example}~\footnote{The equation numbers are rounded for simplicity; as an example the biggest class will have a smaller loss compared to the smallest class. We divided by constant, which is the sum of classesâ€™ density}.

\begin{subequations}
 \begin{align}
 w_c &= \frac{\frac{1}{288}}{\sum\frac{1}{416428}+\frac{1}{370116}\dots+\frac{1}{288}}\\
  &= \frac{\frac{1}{288}}{0.00535} = ~0.03 \\
  &= \frac{\frac{1}{416428}}{0.00535} = ~0.0004
 \end{align}\label{eq:training_weighted_fun_example}
\end{subequations}

\clearpage

\section{Experiments}

\section{Hardware}

We used a Dell Precision T7600 Workstation to conduct our experiments with Intel Xeon E5-2650 32x 2.8GHz CPU, 32GB RAM, 1 NVIDIA GeForce GTX 1080 ti GPU\footnote{\textit{GPU ASUS ROG STRIX GeForce GTX 1080 Ti Assassin's Creed Origins Edition AC-ORIGINS-ROG-STRIX-GTX1080TI, Memory Type: GDDR5X, Connectors: DisplayPort Output, DVI Output: HDMI Standard Output, Chipset/GPU Manufacturer: NVIDIA, Brand: ASUS NVIDIA GeForce GTX 1080 Ti, Compatible Port/Slot: PCI Express 3.0, Memory Size: 11GB 352-Bit GDDR5X, Core Clock 1594 MHz (OC Mode), 1569 MHz (Gaming Mode (Default)), Boost Clock 1708 MHz (OC Mode), 1683 MHz (Gaming Mode (Default)), 1 x DL-DVI-D 2 x HDMI 2.0 2 x DisplayPort 1.4, 3584 CUDA Cores.}}.

More details are available in the following website link: \href{https://www.asus.com/us/Graphics-Cards/AC-ORIGINS-ROG-STRIX-GTX1080TI/}{asus website}
Hard disk SSD 256; with Ubuntu OS, x86\_64 Linux 16.04 LTS. We must emphasize that\footnote{we found a major impact using SSD hard disk when data reading}. Beside the effect of the GPU for Deep Learning experiments, we utilized the memory and the processors to prepare the batches for the input model.

\section{Software}

During our Model development we used the following software and libraries,

\begin{itemize}
 \item Python 3.7: \textit{Used as main programming language.}
 \item Tensorflow: \textit{Used as Deep learning backend framework}
 \item Keras: \textit{Used as High level framework on top of the backend}
 \item Pyarabic: \textit{Used in data pre-processing and cleansing.}
 \item pandas: \textit{Used in data pre-processing and splitting.}
 \item sklearn: \textit{Used to encode the classes using Label-Encoder and for model assessment phase.}
 \item pickle: \textit{Used to save the encoder and the model as serialized pickle object.}
 \item h5py: \textit{Used to save the encoded dataset matrix in h5 format.}
\end{itemize}

\section{Implementation Outline}

\begin{enumerate}
 \item Data Reading and Cleansing:
 \begin{itemize}
 \item We set the random seed and \textit{Numpy} seed in the code to be reproducible.
 \item We used PyArabic~\cite{Pyarabic_2010} to trim and strip some dummy letters~\footnote{The data has some dummy letters named Tatweel for example \textarabic{Ø´Ø¹Ù€Ù€Ù€Ø±} is similar to \textarabic{Ø´Ø¹Ø±} so we removed this letter as it has no meaning.}.
 \item We cleaned the data from any dummy char using Pandas and Numby libraries.
 \item We obtained the maximum Bayt length for padding~\footnote{Padding used zeros, which are not selected Arabic Char representations}.
 \item We removed the dummy letters or wrong diacritics letters.
 \item We factorize \textit{Shadaa and Tanween} to their original letters as explained in Chapter~\nameref{Ch:Background}.
 \item We divided the data into Full (i.e. Including all the Meters) and Eliminated (i.e. Removing the meters which have less than 1\% of the total data).
 \end{itemize}
 
 \item Data Encoding:
 \begin{itemize}

 \item We encoded the Meters label using the label encoder in \textit{Sklearn} and generated Full and Eliminated to output label encoder.
 \item We save the encoder serialized in the desk for later usage.
 \item We used \textit{Pickle} Python library to save the serialized object with \textit{H5 format} in \textit{h5py} library.
 \item We used three methods of encoding \textit{one-hot}, \textit{binary} and \textit{two-hot}, all of them developed using custom Python functions.
 \item We saved the data for each encoding with two combinations: \textit{Full/Eliminated and With/Without \textit{tashkeel}}. \textit{tashkeel}.
 \item We saved these as \textit{h5} format in the desk.
 \end{itemize}

 \item Model Training:
 \begin{itemize}
 \item We used \textit{Keras with Tensorflow} backend as Deep Learning Framework.
 \item We created a custom function to handle the Data combinations \textit{Full/Eliminated and With/Without \textit{tashkeel}} with the Network combinations \textit{Layers, Units and cell type (Bi-LSTM or LSTM)}.

 \item We faced a performance issue while training LSTM for Recurrent Neural Networks as we have a huge amount of data on the current known behaviour of RNN, as it is recurrent. We therefore took steps to reduce our RNN training time by following the next steps:
 \begin{itemize}
  \item Saving the data cleaned and encoded in the hard disk, and reading it directly encoded (as explained in the previous steps). The data was read in batches and the hardware utilized to train the dataset in parallel with these.
  \item We used Nvidia Cuda optimized LSTM cell, which significantly reduced our training time by up to 6x speedup.
  \item We used a parallel GPU in our experiments to test the effect, by utilizing a Keras utils multi\_gpu\_model option.
  \item We saved all the model output for later validations.
 \end{itemize}
 \item We saved all the model output for later validations.

 \end{itemize} 

 \item Results and Validations
 \begin{itemize}
 \item We calculated the confusion matrix on the testing data to calculate the results and carry out the validations.
 \item We calculated the accuracy from the confusion matrix which was similar to the accuracy generated from \textit{Tensorflow} on the testing data.
 \item We used weighted accuracy to check the accuracy for Per-Class.

 \end{itemize}
 
\end{enumerate}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../master"
%%% TeX-engine: xetex
%%% End:
