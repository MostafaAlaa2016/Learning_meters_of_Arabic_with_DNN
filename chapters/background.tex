\chapter{\uppercase{Background}}

Each Arabic letter represents a consonant, which means that short vowels are not
represented by the 36 characters, for this reason, the need of \textit{diacritics}
rises. \textit{Diacritics} are symbols that comes after a letter to state the
short vowel accompanied by that letter. There are four diacritics \textarabic{◌َ} \textarabic{◌ُ}
\textarabic{◌ِ} \textarabic{◌ْ} which represent the following short vowels
/\textit{a}/, /\textit{u}/, /\textit{i}/ and \textit{no-vowel} respectively,
their names are \textit{fat-ha, dam-ma, kas-ra and sukun} respectively.  The first
three symbols are called \textit{harakat}. Table \ref{tables:diacritics_dal}
shows the 4 diacritics on a letter.



% table: dal with diacritics
\begin{table}[H]
	\centering
	\begin{tabular}{c c c c c c}
		%\hline
		\toprule
		\textbf{\small{Diacritics}}     & \small{\textit{without}} & \small{\textit{fat-ha}} &
		\small{\textit{kas-ra}} & \small{\textit{dam-ma}} & \small{\textit{sukun}}\\
		%\hline
		\midrule
		\textbf{\small{Shape}}   & \textarabic{د} & \textarabic{دَ} & \textarabic{دِ} &
		\textarabic{دُ} & \textarabic{دْ}\\
		%\hline
		\bottomrule
	\end{tabular}
	\caption{\textit{Diacritics on the letter  \textarabic{ د }}}\label{tables:diacritics_dal}
\end{table}



There are two more sub-diacritics made up of the basic four to represent two
cases:
\begin{definition}\label{def:shadaa_definition}
  \textbf{Shadaa}  \hfill \\
to indicate the letter is doubled. Any letter with
shaddah (\textarabic{ ّ } ) the letter should be duplicated: first letter with a
constant (sukoon) and second letter with a vowel (haraka) \cite{Alnagdawi2013}; Table  \ref{tables:shadda_dal}
shows the dal with shadda and the original letters.
% table: dal with shadda

\begin{table}[H]
	\centering
	\begin{tabular}{c c c}
		%\hline
		\toprule
		\textbf{\small{Diacritics}} & \small{\textit{letter with Shadda }} & \small{\textit{letters without shadaa  }} \\
		%\hline
		\midrule
		\textbf{\small{Shape}}  & \textarabic{دَّ} &  \textarabic{دْدَ}\\
		%\hline
		\bottomrule
	\end{tabular}
	\caption{\textit{Shadaa diacritics on the letter  \textarabic{ د }}}\label{tables:shadda_dal}
\end{table}

\end{definition}

\begin{definition}\label{def:tanween_definition}
  \textbf{Tanween} \hfill \\
  %%% \ref{defa} and \ref{defb}
  is doubling the short vowel, and can convert
Tanween fathah, Tanween dhammah or Tanween kasrah by
replacing it with the appropriate vowel ( ُ◌ – dhammah, َ◌ –
fathah or ِ◌ –kasrah ) then add the Noon letter with constant to the end of the word \cite{Alnagdawi2013}. Table \ref{tables:Tanween_dal}
shows the difference between the original letter and the letter with Tanween

\begin{table}[H]
	\centering
	\begin{tabular}{c c c}
		%\hline
		\toprule
		\textbf{\small{Diacritics}} & \small{\textit{letter with tanween }} & \small{\textit{letters without tanween}} \\
		%\hline
		\midrule

          \textbf{\small{Tanween Fat-ha}}  & \textarabic{دً} &  \textarabic{دَ+نْ}\\
          \textbf{\small{Tanween Dam-ma}}  & \textarabic{دٌ} &  \textarabic{دُ+نْ}\\
          \textbf{\small{Tanween Kas-ra}}  & \textarabic{دٍ} &  \textarabic{دِ+نْ}\\


		\bottomrule
	\end{tabular}
	\caption{\textit{Tanween diacritics on the letter  \textarabic{ د }}} \label{tables:Tanween_dal}
\end{table}


\end{definition}

 Arabs pronounce the sound \textit{/n/} accompanied \textit{sukun} at the end the indefinite words, that sound corresponds to this
letter \textarabic{نْ}, it is called \textit{noon-sakinah}, however, it is
just a phone, it is not a part of the indefinite word, if a word comes as a
definite word, no additional sound is added. Since it is not an essential sound,
it is not written as a letter, but it is written as  \textit{tanween}
\textarabic{◌ٌ ◌ً ◌ٍ}.
% adding tanween and its relationship to the previous letter
\textit{Tanween} states the sound \textit{noon-sakinah}, but as you have noticed,
there are 3 \textit{tanween} symbols, this because  \textit{tanween} is added as
a diacritic over the last letter of the indefinite word, one of the 3 harakat\textit{harakat} accompanies the last letter, the last letter's \textit{harakah}
needs to be stated in addition to the sound \textit{noon-sakinah}, so
\textit{tanween} is doubling the last letter's \textit{haraka}, this way the last
letter's \textit{haraka} is preserved in addition to stating the sound
\textit{noon-sakinah}; for example, \textarabic{رَجُلُ + نْ} is written
\textarabic{رَجُلٌ} and  \textarabic{رَجُلِ + نْ} is written \textarabic{رَجُلٍ}.


Those two definition, Definition ~\ref{def:shadaa_definition} and Definition ~\ref{def:tanween_definition}  will help us to reduce the dimension of the letter's feature vector as we will see in \textit{preparing data} section.


Diacritics makes short vowels clearer, but they are not necessary.
Moreover, a phrase without full diacritics or with just some on some letters is
right linguistically, so it is allowed to drop them from the text.

% Diacritics in Unicode
In Unicode, Arabic diacritics are standalone symbols, each of them has its own
unicode. This is in contrast to the Latin diacritics; e.g., in the set
\textit{\{ê, é, è, ë, ē, ĕ, ě\}}, each combination of the letter \textit{e} and a diacritic is represented by one unicode.

\newpage

\section{Arabic Arud Science}
% @@@ add el3elal and zahafat
% @@@ add tent picture
% @@@ add details into every bahr



\begin{definition}\label{def:arud}
  \textbf{Arud} \hfill \\
  In Arabic Arud natively has many meanings (the way, the direction, the light clouds and Mecca and Madinah \footnote{\textit{Mecca and Madinah are two cities in  Saudi Arabia}.}\cite{AlQuaed}. Arud is the science which studies The Arabic Poem meters and the rules which confirm if the Poem is sound meters \& broken meters. If we need to su
\end{definition}
 
The Author of this science is \textit{Al-Farahidi} (718 – 786 CE) has analyzed the
Arabic poetry; then he came up with that the succession of consonants and vowels
produce patterns or \textit{meters}, which make the music of poetry. He was one of the famous people who know The melodies and the musical parts of speech. He has
counted them fifteen meters.  After that, a student of \textit{Al-Farahidi} has
added one more meter to make them sixteen. Arabs call meters \textarabic{بحور}
which means "\textit{seas}" Poets have written poems without knowing exactly what rules which make a collection of words a poem.

The Reasons which makes \textit{Al-Farahidi} put this science is

  \begin{itemize}
  \item Protect the Arabic Poems from the broken meters.
  \item Distinguish between the original Arabic Poem and the non-poem or from the prose.
    \item Make the rules clear and easy for anyone who needs to write a poem.
  \end{itemize}

  Some people said that the one-day Al-Farahidi was walking into the metal-market and he was said some of the poems and for some reasons the knock of the metals matched the musical sound of the poem he was saying then he got an idea to explore the Arud of the poems.

 There are many reason for this science name 
  \begin{itemize}
  \item It named Arud because some people said he put this science in Arud place \textarabic{العَروض} \textit{with fat-ha, not with dam-ma such as the science name \textarabic{العُروض} } between Mecca and Al-Ta'if\cite{AlQuaed}.
  \item Arud in Arabic is noun come from verb \textarabic{يعرض} which means here to be assessed. They said because of Any poem should be assessed by Al-Arud science so, it named Al-Arud \cite{Alkafi1994}.
  \end{itemize}
  
  

  
    \newpage
    \subsection{Al-Farahidi and Pattern Recognition}
    This subsection is our opinion in Al-Farahidi and his method he followed during working on Arabic Poem Classifications.

\begin{enumerate}


\item Al-Farahidi thought there is a pattern for every collection of the poem by chance; however, He scientifically worked into this problem. He started analyzing the poem and add every group with the same tafa'il to the same class.
\item He analyzed the outliers and the particular case from every class and added it to his model.
\item He revised the Bohor and get the cases and generalize his case to be fit into all Poems.
\item His student once he found some Poems which weren't fit into any model to be a model for a new class.

\end{enumerate}
The best essential point which made us admired by Al-Farahidi is his way of research and his passion for getting an indeed succession model. Also, his model is general and followed all the steps currently any Data scientist follows to explore new pattern. Some people state that He died when he was thinking about the problem he hit a wall which made trouble for him. His die story shows that he was thinking in profoundly about this problem. One of the most interest thing I found during this research is how he found this pattern and Al-Farahidi’s way to find a new thing.
%@@@ add about student for sebayeah
%@@@ التأكد من أن القرآن والحديث ليس شعر وﻹن تصادف فهو ليس قصداً والشعر يجب أن يكون وزناً إتفاقاً

    \newpage
    
    \subsection{Feet Representation}
    A meter is an ordered sequence of feet. Feet are the basic
units of meters; there are ten of them.
\begin{definition}\label{def:feet}
  \textbf{Feet} \hfill \\  A Foot consists of
a sequence of \textbf{Sukun} (Consonants) represented as (0) and \textbf{Harakah} (Vowels) (/). Traditionally, feet are represented by mnemonic words called tafa’il \textarabic{تفاعيل}.
\end{definition}

Feets consists of three parts (Reasons \textarabic{أسباب}, Wedge \textarabic{وتد}, Breaks \textarabic{فواصل}).
\begin{itemize}
\item \textbf{Reasons (\textarabic{أسباب})}: It has two types
  \begin{enumerate}
  \item \textbf{Light (\textarabic{سبب خفيف})} which happens when we have the first letter is harakah and the second is sukun (/0) example (\textarabic{هَبْ, لَمْ}).
    \item \textbf{Heavy (\textarabic{سبب ثقيل})} which happens when we have two harakah letter (//) example (\textarabic{لَكَ, بِكَ}).
    \end{enumerate}
    \item \textbf{Wedge (\textarabic{وتد})}: It has two types
  \begin{enumerate}
  \item \textbf{Combined Wedge (\textarabic{وتد مجموع})} which happens when we have two harakah letters followed by sukun (//0) example (\textarabic{مَشَى, عَلَى}).
    \item \textbf{Separated Wedge (\textarabic{وتد مفروق})} which happens when we have two harakah and in between a sukun letter (/0/) example (\textarabic{مُنْذُ, مِصْرُ}).
    \end{enumerate}
    \item \textbf{Breaks (\textarabic{فواصل}}): It has two types
  \begin{enumerate}
  \item \textbf{Small Break (\textarabic{فاصلة صغرى}}) which happens when we have three harakah letters followed by a sukun letter (///0) example (\textarabic{ذَهَبُوا, سُفُناً}).
    \item \textbf{Big Break (\textarabic{فاصلة كبرى}}) which happens when we have four harakah letters followed by a sukun letter  (////0) example (\textarabic{جَعَلَهُمْ}).\footnote{\textit{Some of Arab linguistic scientist assume the small Breaks as a combination between big reason and small reason. Same for the Big Breaks assumed to be a combination between Big reason and Combined Wedge. So, they didn't assume we have three types of feet it is only pure two and any other feets constructed from this two. In this thesis we assume there are three feets }.}
    \end{enumerate}
  \end{itemize}

\newpage
  \subsubsection{Rules for Arabic Letters Representation}
  Arabic Arud has one general rule in the poem representation which is we represent only the letters which is (spoken) not the written which means the letters with phonatics not the written. We have give the below rules as a results of the general rule.

  \begin{itemize}
  \item Any letter with \textit{harakah} represented as (/).
  \item Any letter with \textit{sukun} represented as (0).
  \item Any letter with shaddah represented by two letters the first one will be \textit{sukun} and the second letter will be \textit{harakah} represented as (0/) example (\textarabic{مُحَمََّد}) will be (//0//0).
  \item Any letter with tanween represented by two letters the first one is \textit{haraka} (/) and the second is \textit{sukun}.
  \item Alef without hamze (\textarabic{همزة الوصل}) and Wow Algmaa are not represented example (\textarabic{وُاعلَموا}) will be (/0//0)
  \item If we have a letter which is not written but (spoken) so, we will represent it example (\textarabic{هذا}) it include Alef but not written (\textarabic{هاذا}) the representation will be (/0/0).
  \item If we have \textit{Meem Aljamaa} with harakah so, it represented with \textit{Mad} example (\textarabic{هُمُ}) will be (//0) .
  \item \textit{Alef Mad} (\textarabic{آ}) will be two letters \textit{Alef with harakah} and \textit{Alef with sukun} example (\textarabic{آدَمُ}) will be (/0//).
    \item if the verse ended with \textit{harkah} we will add \textit{sukun} to it.


    \end{itemize}
Example: (note: the below representation first line is simliar the second one but with Arud language style ).
\begin{Arabic}
  \begin{traditionalpoem*}

    أرَاكَ عَصِيَّ الدّمعِ شِيمَتُكَ الصّبرُ، *** أما للهوى نهيٌ عليكَ ولا أمرُ ؟
     أرَاكَ عَصيْيَ دّمعِ شِيمَتُكَ صّبرُو، ***  أما للهوى نهينْ عليكَ ولا أمرو ؟
    
	\end{traditionalpoem*}
\end{Arabic}
%@@@ add example the difference between arud writting and the actual writing
\newpage

\subsection{Arabic Poetry Feets}

Arabic poetry feets has ten tafa'il \textarabic{تفاعيل} (scansion)  any peom constructed from these feets. They are eight from writing (syntax) perspective, But it ten in the rules.
\begin{savenotes}

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{\#} & \textbf{Feet} & \textbf{Scansion} & \textbf{Construction} \\
    \hline
    1 & \textarabic{فَعُولُنْ}  & \texttt{0/0//} & combined wedge (\textarabic{فعو}) and small reason (\textarabic{لن})   \\
    2 &\textarabic{مَفاعِيلُنْ}& \texttt{0/0/0//} & combined wedge (\textarabic{مفا}) and two light reasons (\textarabic{عي}) (\textarabic{لن})   \\
    3 &\textarabic{مُفَاعَلَتُنْ}& \texttt{0///0//}  &    combined wedge (\textarabic{مفا}), heavy reason (\textarabic{عل}) and light reason (\textarabic{تن}) \\
    4 &\textarabic{فَاعِلاَتُنْ} & \texttt{0/0//0/}   & light reason (\textarabic{فا}), combined wedge (\textarabic{علا}) and light reason (\textarabic{تن})   \\
    5 &\textarabic{فَاعِ لاتُنْ} & \texttt{0/0//0/}  &  Separated wedge (\textarabic{فاع}) and two light reason (\textarabic{لا})(\textarabic{تن}) \footnote{\textit{We separated the letters (\textarabic{ع}) and (\textarabic{لا}) in (\textarabic{فاع لاتن}) to show that this part is separated wedge and distinguish between this feet  and (\textarabic{فاع لاتن}) which contains combined wedge  }.}  \\
    6 &\textarabic{فَاعِلُنْ}  & \texttt{0//0/}   & light reason (\textarabic{فا}) and combined wedge (\textarabic{علن})\\
    7 &\textarabic{مُتَفَاعِلُنْ}& \texttt{0//0///}  & heavy reason (\textarabic{مت}), light reason (\textarabic{فا}) and combined wedge (\textarabic{علن})  \\
    8 &\textarabic{مَفْعُولاَت} & \texttt{0//0///}   & two light reason (\textarabic{مف})(\textarabic{عو}) and separated wedge (\textarabic{لات}) \\
    9 &\textarabic{مُسْتَفْعِلُنْ} & \texttt{0//0/0/}  &  two light reason (\textarabic{مس})(\textarabic{تف}) and combination wedge (\textarabic{علن}) \\
    10 &\textarabic{مُسْتَفْعِ لُنْ} & \texttt{0//0/0/}  & light reason (\textarabic{مس}), separated wedge  (\textarabic{تفع}) and light reason  (\textarabic{لن})\footnote{\textit{We separated the letters (\textarabic{ع}) and (\textarabic{ل}) in (\textarabic{مستفع لن}) to show that it ends with a separated wedge and distinguish between this feet  and (\textarabic{مستفعلن}) which contains combined wedge }}\\


    \hline
  \end{tabular}
  \caption{The ten feet of the Arabic meters. }\label{arud:feet}
\end{table}
    \end{savenotes}

%% Every digit (\texttt{/} or \texttt{0}) represents
%%    the corresponding diacritic over a letter in the feet. \texttt{/} corresponds to
%%    a\textit{harakah} ( \textarabic{◌َ}, \textarabic{◌ُ}, or \textarabic{◌ِ}) and \texttt{0}
%%    corresponds to a \textit{sukun} (\textarabic{◌ْ}). Any \textit{mad} (\textarabic{و, ا, ى}) is
%%    equivalent to \texttt{0}, \textit{tanween} is equivalent to \texttt{0/}, and \textit{shaddah} is
%%    equivalent to \texttt{/0}%

    \newpage
\begin{definition}\label{def:meter}
  \textbf{Meter} \hfill \\
  %%%% What is rtythm,feet
  Poetic meters define the basic rhythm of the poem. Each meter is described by a set of ordered feet which can
be represented as ordered sets of consonants and vowels \cite{Almuhareb2015}.

% \textbf{Some conventions and terminologies}:

% What are poems and terminologies?
% What does a poem look like? bayt, shatr, ....
\begin{Arabic}
	\begin{traditionalpoem*}
          ولد الهدى فالكائنات ضياء *** وفم الزمان تبسم وثناء انشاء
          الروح والملأ الملائك حوله *** للدين والدنيا به بشراء

	\end{traditionalpoem*}
\end{Arabic}%



\end{definition}


\begin{definition}\label{def:verse}
  \textbf{Arabic Verse} \hfill \\ refers to "poetry" as contrasted to prose. Where the common unit of a verse is based on meter or rhyme, the common unit of prose is purely grammatical, such as a sentence or paragraph \footnote{\textit{ https://en.wikipedia.org/wiki/Verse\_(poetry)}.}. A verse know as \textit{Bayt} in Arabic \textarabic{بيت}

\end{definition}


\begin{definition}\label{def:shatr}
  \textbf{Shatr} \hfill \\  A verse consists of two halves, each of them is called \textit{shatr} and carries the full meter.  We will use the term \textit{shatr} to refer to a verse's half; whether the right or the left half.
\end{definition}



\begin{definition}\label{def:poem}
  \textbf{Poem} \hfill \\
  is a set of verses has the same meter and rhyme.

\end{definition}


\newpage

\subsection{Arabic Poetry Meters}

\subsubsection{Al-Taweel \textarabic{الطويل}}
\textbf{Why it named Al-Taweel?}
\textit{Al-Taweel is named Al-Taweel for two reasons; first, It is the longest meter between all meters. Second, It starts with Wedge then Reasons and Wedge is longer than Reasons. So, it named Al-Taweel. We need here to note later in the encoding section we will pad all other meters by zeros to make it all the same length. Example if the max Bayt is 82 so, any Bayt less than 82 will be padded by zeros to have the same length.\cite{Alkafi1994}
  }\\

\textbf{tafa'il}

\begin{Arabic}
	\begin{traditionalpoem*}
          فعلون مفاعيلن فعولن مفاعلن *** فعولن مفاعيلن فعولن مفاعيلن


	\end{traditionalpoem*}
      \end{Arabic}

\textbf{Example:}
\begin{Arabic}
	\begin{traditionalpoem*}
    %$> العصر العباسي >> ابن الفارض
          إذا جادَ أقوامٌ بِمالٍ رأيْتَهُمْ  *** يَجودونَ بالأرواحِ مِنْهُم بِلا بُخلِ
          //0/0 //0/0/0 //0/0 //0//0 *** //0/0 //0/0/0 //0/0 //0/0/0
          فعلون مفاعيلن فعولن مفاعلن *** فعولن مفاعيلن فعولن مفاعيلن

	\end{traditionalpoem*}
      \end{Arabic}

% % % 	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\subsubsection{Al-Madeed \textarabic{المديد}}
\textbf{Why it named Al-Madeed?}

\textit{Al-Madeed is named because of the reasons \textarabic{الأسباب} is represented in all its seven parts of tafa'il, One in the first part and the other in the second part. So, it named Madeed \textarabic{مديداً}\cite{Alkafi1994}
  }\\

% @@@note to add references

\textbf{tafa'il}

\begin{Arabic}
	\begin{traditionalpoem*}
فاعلاتن فاعلِن فاعلاتن *** فاعلاتن فاعلن فاعلاتن


	\end{traditionalpoem*}
      \end{Arabic}


\textbf{Example:}

\begin{Arabic}
	\begin{traditionalpoem*}
    % % % العصر الجاهلي >> المهلهل بن ربيعة - الزير
    يَا لِبَكْرٍ أَنْشِرُوا لِي كُلَيْباً	يَا لِبَكْرٍ أَيْنَ أَيْنَ الْفِرَارُ
/0//0/0 /0//0 /0//0/0 *** /0//0/0 /0//0 /0//0/0
فاعلاتن فاعلِن فاعلاتن *** فاعلاتن فاعلن فاعلاتن

	\end{traditionalpoem*}
      \end{Arabic}
\newpage



% % % 	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

%@@@ check bohor name is same for all latex files
\subsubsection{Al-Baseet \textarabic{البسيط}}

\textbf{Why it named Al-Baseet?}
Al-Baseet there is a different idea behind this name
\begin{itemize}
\item The Reasons \textarabic{الأسباب} expanded into it is tafa'il. So, We will find at the beginning of every part two reasons so; it named Al-Baseet.
\item The other reasons which may be the more logic are the harkat  \textarabic{الحركات} expanded in its tafa'il.\cite{Alkafi1994}
  
\end{itemize}



\textbf{tafa'il}

\begin{Arabic}
	\begin{traditionalpoem*}
مستفعلن فعلن مستفعلن فاعلن *** مستفعلن فاعلن مستفعلن فاعلن


	\end{traditionalpoem*}
      \end{Arabic}


\textbf{Example:}

\begin{Arabic}
	\begin{traditionalpoem*}
          % % % العصر الجاهلي >> المهلهل بن ربيعة - الزير

لَيْسَ الجَمَال بأَثْوابٍ تُزَيِّنُنَا    ***	إن الجمال جمال العلم والأدب
/0/0//0 ///0 /0/0//0 ///0   ***  /0/0//0 ///0 /0/0//0 ///0
مستفعلن فعلن مستفعلن فاعلن  *** مستفعلن فاعلن مستفعلن فاعلن
	\end{traditionalpoem*}
      \end{Arabic}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\subsubsection{Al-Wafer \textarabic{الوافر}}
\textbf{Why it named Al-Wafer?}
Al-Wafer there are different ideas behind this name
\begin{itemize}
\item There is much harakat in its parts because there is no tafa'il has part includes harakat more than the word \textarabic{مفاعلتن}.
\item There are many parts to its base.\cite{Alkafi1994}
\end{itemize}

\textbf{tafa'il}

\begin{Arabic}
	\begin{traditionalpoem*}

          مفاعلتن مفاعلتن مفاعلتن *** مفاعلتن مفاعلتن مفاعلتن

	\end{traditionalpoem*}
      \end{Arabic}


\textbf{Example:}

\begin{Arabic}
  \begin{traditionalpoem*}
    %%% الأولى >> العصر الجاهلي >> عمرو بن كلثوم >> أَلاَ هُبِّي بِصَحْنِكِ فَاصْبَحِيْنَا ( معلقة )
    إِذَا بَلَـغَ الفِطَـامَ لَنَا صَبِـيٌّ *** تَخِـرُّ لَهُ الجَبَـابِرُ سَاجِديْنَـا
    //0///0 //0///0 //0/0 ***  //0///0 //0///0 //0/0
    مفاعلتن مفاعلتن فعولن  *** مفاعلتن مفاعلتن فعولن

	\end{traditionalpoem*}
      \end{Arabic}
\newpage

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\subsubsection{Al-Kamel \textarabic{الكامل}}
\textbf{Why it named Al-Kamel?}
\textit{Al-Kamel is named because its harakat is fully integrated and it is 30 harakah which is not similar to any other Bahr has this numbers of harakat. However, Al-wafer has much harakat in its parts but not the same number as Al-Kamel. Al-Wafer has the harakat but it not overwritten into its source but Al-Kamel it is written into its source \textarabic{أصله}. So, Al-Kamel in Arabic named due to it is more integrated  \textarabic{أكمل} than Al-Wafer \cite{Alkafi1994}.
}\\

\textbf{tafa'il}

\begin{Arabic}
	\begin{traditionalpoem*}
متفاعلن متفاعلن متفاعلن *** متفاعلن متفاعلن متفاعلن


	\end{traditionalpoem*}
      \end{Arabic}


\textbf{Example:}

\begin{Arabic}
	\begin{traditionalpoem*}
    % % % الأولى >> العصر الجاهلي >> عنترة بن شداد >> هلْ غادرَ الشُّعراءُ منْ متردَّم ( معلقة )
وَلَقَدْ شَفَا نَفْسِي وَأَبْرَأَ سُقْمَهَا *** قِيلُ الفَوَارِسِ وَيكَ عَنْتَرَ أَقْدِمِ
///0//0 /0/0//0 ///0//0 ***  /0/0//0 ///0//0 ///0//0
متفاعلن مستفعلن متفاعلن *** مستفعلن متفاعلن متفاعلن

	\end{traditionalpoem*}
      \end{Arabic}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\subsubsection{Al-Hazaj \textarabic{الهزج}}
\textbf{Why it named Al-Hazaj?}
\textit{Al-Hazaj is named because of the sound reverberation in its parts. Al-Hazaj in Arabic is sound frequency. So, due to the sound reverberation, it named AL-Hazaj. Also, Because every part ends for two reasons so, it made some of sound like a piece of music to be Al-Hazaj \cite{Alkafi1994}.  }\\

\textbf{tafa'il}

\begin{Arabic}
	\begin{traditionalpoem*}
مفاعيلن مفاعيلن *** مفاعيلن مفاعيلن


	\end{traditionalpoem*}
      \end{Arabic}


\textbf{Example:}

\begin{Arabic}
	\begin{traditionalpoem*}
%%%%أدب .. ابن عبد ربه
          أَيَا مَنْ لاَمَ في الحُبِّ *** وَلَمْ يَعْلَمْ جَوى قَلبي
          //0/0/0 //0/0/0 *** //0/0/0 //0/0/0
          مفاعيلن مفاعيلن *** مفاعيلن مفاعيلن




	\end{traditionalpoem*}
      \end{Arabic}


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\subsubsection{Al-Rejz \textarabic{الرجز}}
\textbf{Why it named Al-Rejz?}
\textit{Al-Rejz is named Al-Rezj because it constructed from three parts. If there is an animal and someone pull this animal by one leg and the animal walk into three legs in Arabic named Rejz \textarabic{رجزاً}, this is the reason it named Rejz because of it has three parts similar than animal pulled by one leg and walk into three legs. Also, in Arabic, if we have a camel when it stands up has some disturbances due to it sick or has any issue it named Rejz Camel \textarabic{جمل رجز}, and in Arabic disturbance \textarabic{إضطراب} means Rejz for example \cite{Alkafi1994}.}\\


\textbf{tafa'il}

\begin{Arabic}
  \begin{traditionalpoem*}
    مستفعلن مستفعلن مستفعلن *** مستفعلن مستفعلن مستفعل


	\end{traditionalpoem*}
      \end{Arabic}


\textbf{Example:}

\begin{Arabic}
  \begin{traditionalpoem*}
    دار لسلمى إذ سليمى جارة *** قفر ترى آيانها مثل الزّبر
    /0/0//0 /0/0//0 /0/0//0 *** /0/0//0 /0/0//0 /0/0//0
مستفعلن مستفعلن مستفعلن *** مستفعلن مستفعلن مستفعلن



	\end{traditionalpoem*}
      \end{Arabic}
      % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


\subsubsection{Al-Raml \textarabic{الرمل}}
\textbf{Why it named Al-Raml?}
\textit{Al-Raml is named Al-Raml because Al-Raml is a type of the singing which constructed from this Bahr. Another reason is the Wedges is appeared in between the Reasons so; it named Al-Raml due to the diversity between the Reasons and the wedges inside the tafa'il \cite{Alkafi1994}.}\\

\textbf{tafa'il}

\begin{Arabic}
  \begin{traditionalpoem*}

    فاعلاتن فاعلاتن فاعلاتن *** فاعلاتن فاعلاتن فاعلاتن


	\end{traditionalpoem*}
      \end{Arabic}


\textbf{Example:}

\begin{Arabic}
  \begin{traditionalpoem*}

    إنما الدنيا غرور كلها *** مثل لمغ اﻵل في أرض الفقار
    /0//0/0 /0//0/0 /0//0 *** /0//0/0 /0//0/0 /0//0/0
    فاعلاتن فاعلاتن فاعلن *** فاعلاتن فاعلاتن فاعلاتن


	\end{traditionalpoem*}
      \end{Arabic}


      % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


\subsubsection{Al-Sarea \textarabic{السريع}}
\textbf{Why it named Al-Sarea?}
\textit{Al-Sarea is named Al-Sarea in Arabic Al-Sarea means the fastest. It named these because its speed in teste \textarabic{الزوق} or its parts\textarabic{التقطيع}. The other reason because its three parts have 7 reasons and the reasons are faster than the wedges.\cite{Alkafi1994}}\\

\textbf{tafa'il}

\begin{Arabic}
  \begin{traditionalpoem*}

مستفعلن مستفعلن مفعولات *** مستفعلن مستفعلن مفعولات

	\end{traditionalpoem*}
      \end{Arabic}


\textbf{Example:}

\begin{Arabic}
  \begin{traditionalpoem*}


    أزمان سلمى لا يرى مثلها الر *** راؤون في شام ولا عراق
/0/0//0 /0/0//0 /0//0 *** /0/0//0 /0/0//0 /0//00
مستفعلن مستفعلن مفعولات *** مستفعلن مستفعلن مفعولات



	\end{traditionalpoem*}
      \end{Arabic}



      % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


\subsubsection{Al-Monsareh \textarabic{المنسرح}}
\textbf{Why it named Al-Monsareh?}
\textit{to be written later :) :) \cite{Alkafi1994}.}\\

\textbf{tafa'il}

\begin{Arabic}
  \begin{traditionalpoem*}

مستفعلن مفعولات مستفعلن *** مستفعلن مفعولات مستفعلن

	\end{traditionalpoem*}
      \end{Arabic}


\textbf{Example:}

\begin{Arabic}
  \begin{traditionalpoem*}

    إن ابن زيد لا زال مستعملا *** للخير يفشى فى مصره العرفا
    /0/0//0 /0/0/0/ /0/0//0 *** /0/0//0 /0/0/0/ /0///0
    مستفعلن مفعولات مستفعلن *** مستفعلن مفعولات مفتعلن



	\end{traditionalpoem*}
      \end{Arabic}


      \newpage
      % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


\subsubsection{Al-Khafeef \textarabic{الخفيف}}
\textbf{Why it named Al-Khafeef?}
\textit{Al-Khafeef name in Arabic means light. The reason behind the name is its wedge last harkah connected to its reason so, it became light \textarabic{خفت}. The other reason is its light in teste \textarabic{الزوق} or its parts \textarabic{التقطيع} because it has a part with three reasons and the reasons are lighter than wedges\cite{Alkafi1994}. }\\

\textbf{tafa'il}

\begin{Arabic}
  \begin{traditionalpoem*}

    فاعلاتن مستفع لن فاعلاتن *** فاعلاتن مستفع لن فاعلاتن
    %%%%مستفع لن
	\end{traditionalpoem*}
      \end{Arabic}


\textbf{Example:}

\begin{Arabic}
  \begin{traditionalpoem*}


    حل أهلي ما بين دوني فبادو *** لى وحلّت علوية بالسخال
    /0//0/0 /0/0//0  /0//0/0 *** /0//0/0 /0/0//0 /0//0/0
    فاعلاتن مستفع لن فاعلاتن *** فاعلاتن مستفع لن فاعلاتن


	\end{traditionalpoem*}
      \end{Arabic}

            % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


\subsubsection{Al-Modarea \textarabic{المضارع}}
\textbf{Why it named Al-Modarea?}
\textit{Al-Modarea in Arabic means the present. It named by this name because it is the present version of the Al-Hazaj. Also, This Bahr wasn't famous in Arabic Poem and there weren't any popular peams or poetry used this Bahr before \cite{Alkafi1994}. }\\

\textbf{tafa'il}

\begin{Arabic}
  \begin{traditionalpoem*}

مفاعلين فاع لاتن *** مفاعلين فاع لاتن

  \end{traditionalpoem*}
      \end{Arabic}


\textbf{Example:}

\begin{Arabic}
  \begin{traditionalpoem*}


    كأن لم يكن جديراً *** بحفظ الذي أضاعا
    //0/0/  /0//0/0 *** //0/0/  /0//0/0
    مفاعيل  فاع لاتن *** مفاعيل  فاع لاتن


	\end{traditionalpoem*}
      \end{Arabic}
 \newpage
            % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


\subsubsection{Al-Moktadeb \textarabic{المقتضب}}
\textbf{Why it named Al-Moktadeb ?}
\textit{Al-Moktadeb in Arabic means reproduced from another thing \textarabic{الإقتطاع} and because this Bahr word is all appeared into Al-Monsareh in all its words but also there is a difference in the order of the parts. So, it named Al-Moktadeb because it seems to reproduce from Al-Monsareh. We will have another section which will focus on the relation between the Bohor \cite{Alkafi1994}.  % @@@note to add references of the section
}\\


\textbf{tafa'il}

\begin{Arabic}
  \begin{traditionalpoem*}

مفعولات مستفعلن *** مفعولات مستفعلن

  \end{traditionalpoem*}
      \end{Arabic}


\textbf{Example:}

\begin{Arabic}
  \begin{traditionalpoem*}

    حف كأسها الحبب *** فهي فضة ذهب
    /0//0/  /0///0 *** /0//0/  /0///0
    فاعلات  مفتعلن *** فاعلات مفتعلن




	\end{traditionalpoem*}
      \end{Arabic}

      % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


      

      \subsubsection{Al-Mojtaz \textarabic{المجتز}}
\textbf{Why it named Al-Mojtaz?}
\textit{Al-Mojtaz in Arabic name is similar meaning for Al-Moktadeb it means reproduced from another thing \textarabic{الإقتطاع أو الإجتزاز} and because these Bahr words are all appeared into Al-khafeef in all its words but also there is a difference in the order of the parts. So, it named Al-Mojtaz because it seems it reproduced from Al-khafeef \textarabic{إجتُزَّ من بحر الخفيف} \cite{Alkafi1994}.}\\


\textbf{tafa'il}

\begin{Arabic}
  \begin{traditionalpoem*}

    مستفع لن فاعلاتن *** مستفع لن فاعلاتن

	\end{traditionalpoem*}
      \end{Arabic}


\textbf{Example:}

\begin{Arabic}
  \begin{traditionalpoem*}


    أنت الذى ولدتك *** أسماء بنت الحباب
    /0/0//0  ///0/ *** /0/0//0  /0//0/0
    مستفع لن  فعِلاتُ *** مستفع لن فاعلاتن


	\end{traditionalpoem*}
      \end{Arabic}

      \newpage
                  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


\subsubsection{Al-Motaqareb \textarabic{المتقارب}}
\textbf{Why it named Al-Motaqareb?}
\textit{Al-Motaqareb in Arabic means convergent and is named by this name because the wedges are convergent to each other this is because between every two wedges one reason, so the wedges are convergent to each other. Also, another reason is its parts are similar to each other, so it named Al-Motaqareb \cite{Alkafi1994}.
}
\\

\textbf{tafa'il}

\begin{Arabic}
  \begin{traditionalpoem*}
    فعولن فعولن فعولن فعولن *** فعولن فعولن فعولن فعولن


	\end{traditionalpoem*}
      \end{Arabic}

%%%http://www.uobabylon.edu.iq/uobColeges/lecture_view.aspx?fid=19&depid=1&lcid=31904
\textbf{Example:}

\begin{Arabic}
  \begin{traditionalpoem*}

    فأمَّا تَميمٌ، تَميمُ بنُ مُرٍّ *** فألفاهُمُ القَومُ رَوْبَى، نِياما
    //0/0 //0/0 //0/0 //0/0 *** //0/0 //0/0 //0/0 //0/0
    فعولن فعولن فعولن فعولن *** فعولن فعولن فعولن فعولن




	\end{traditionalpoem*}
      \end{Arabic}

                        % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


\subsubsection{Al-Motadarek \textarabic{المتدارك}}
\textbf{Why it named Al-Motadarek?}
There are different ideas behind this name.
\begin{itemize}
  \item Al-Motadarek in Arabic means explored because Al-Farahidi forgets this Bahr and his student Al-Akhfash Al-Awsat has explored it named by this name.
 %%@@@ (215/830)
  \item Al-Motadarek in Arabic also means followed by something \textarabic{يُدرك}, and because this Bahr came after Al-Motaqareb, it names Al-Motadarek \cite{Alkafi1994}.\\
  \end{itemize}

\textbf{tafa'il}

\begin{Arabic}
  \begin{traditionalpoem*}
فَاْعِلُنْ فَاْعِلُنْ فَاْعِلُنْ فَاْعِلُنْ *** فَاْعِلُنْ فَاْعِلُنْ فَاْعِلُنْ فَاْعِلُنْ
	\end{traditionalpoem*}
      \end{Arabic}

      \textbf{Example:}

\begin{Arabic}
 \begin{traditionalpoem*}
    لَمْ يَدَعْ مَنْ مَضَى لِلَّذِيْ قَدْ غَبَرْ *** فَضْلَ عِلْمٍ سَوَى أَخْذِهِ بِالأثَرْ
    /0//0 /0//0 /0//0 /0//0 *** /0//0 /0//0 /0//0 /0//0
    فَاْعِلُنْ فَاْعِلُنْ فَاْعِلُنْ فَاْعِلُنْ *** فَاْعِلُنْ فَاْعِلُنْ فَاْعِلُنْ فَاْعِلُنْ
 \end{traditionalpoem*}
\end{Arabic}
  
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
  \newpage
  \subsection{Bohor Relations}
  to be added :) 
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
  \newpage
  \section{Deep Learning Recurrent Neural Networks}
\textbf{What is Deep Learning?} \textit{ Deep Learning is a new approach of Machine Learning research which focus on learning and understanding from the data without the needs for the human operator to formally specify all the knowledge that the computer needs. This method built using a hierarchy of concept which enables the computer to learn complex concepts by building them layer by layer from simpler ones. If there is a graph which shows how this concept built we will figure out a very deep graph with many layers, for this reason, we call this approach to AI deep learning \cite{Goodfellow-et-al-2016}
      }\\

      There was many of early trials to utilize the AI into real life problems. For Example, IBM's Deep Blue chess-playing system which defeated world champion Garry Kasprov in 1997 ( Hsu , 2002 ).%@@@ add the reference
      \\


      Another approach which used to use AI but using hard-code knowledge about the world informal language. A computer can understand statements from the formal language automatically using logical inference rules. This is known as the knowledge base approach to artificial intelligence rules. None of these projects has achieved significant success. For Example, Cyc is tried to gather a comprehensive ontology and knowledge base about the basic concepts about how the world works Cyc  (Lenat and Guha, 1989). Cyc is an inference engine and a database of statements in a language called Cycl. A staff of human supervisors enters these statements. People struggle to devise formal rules with enough complexity to describe the world accurately\cite{Goodfellow-et-al-2016}.\\

      The difficulty faced in the previous system is due to the hard-coded knowledge has shown up the AI need to acquire their knowledge from the data itself. This capability is known as machine learning. This approach has introduced some algorithms which solve and tackle the problems from which we can, for example, check the email is spam or not. Also, it used for other problems for price predictions for housing Example of this algorithms is (Naive Bayes, Logistic regression).

      This simple machine learning approach is working in the data but not with its original format it required some different representation to be input for the model. This different representation named feature engineering. Feature Engineering example: in case of email spam or not spam example it can be word frequency, char frequency, class attributes, capital letters frequency, some other data processing such as remove stop words from the input lemmatization. So, all the previous feature provided by a human expert which know the problem in details and analyzing which features it affect the data then add it as a feature to the input model.
      \newpage

      However, for many tasks, it is difficult to identify the features which should be extracted. For example, we need to detect cars in photographs. We know every car have wheels. So, to detect cars, we can check if there is a wheel to be a feature for car detection. However, to detect or to describe wheels in terms of pixel values is a difficult task. The image may be not clear or may be complicated by shadows, the sun glaring off the metal parts of the wheel, the blurring in images may not make it clear sometimes, and so on\cite{Goodfellow-et-al-2016}.\\

      One solution to solve this problem is to use machine learning itself to discover not only the output of the model but also the features which are the input for the model. This approach is known as representation learning. Learned representation can achieve better results than hard-designed representation. This approach also allows AI systems to rapidly adapt to new tasks or be automatically identify it from any new data. A representation learning can discover many features automatically fast or can take more times in case complex tasks, but at least it will get an excellent set of features which adapt for any complex problem without the need for manual features. In this research, we used the AI to identify the features for our model which make this model get a breakthrough results than the old fashion of manual feature machine learning used.

      If we go back to the image example, we can show that it is not an easy task to extract features to detect the car from an image. So, Deep learning is trying to solve this problem in feature engineering by introducing representation learning that are build complex representations in terms of another simpler layer of representations Figure \ref{fig:DeepLearningImagePersonExample.png} shows how deep learning represents an image of a person by combining simpler representation example the edges and contours which led to understanding complex representations. The benefit from allowing the computer to understand the data and building the representation is the ability now for building and understanding very complex representation and also, to utilize and combine features from simpler to deep representations with many ways such as recurrent or sequences.

      
Modern deep learning provides a compelling framework for learning data problems. This model becomes more complex by the adding more layers and more units within a layer. Deep Learning model is working perfectly on the big dataset which allows the model to learn the data features in a good way.


In the remaining parts in this section we will start introducing the main concepts and component used in deep learning, Also the basic unit into Recurrent Neural networks and LSTM.

      
\begin{figure}[h!] \includegraphics[width=\linewidth]{./Figures/DeepLearningImagePersonExample.png}
  \caption{Illustrations on how can Deep Learning work based on images figure presented from \cite{Goodfellow-et-al-2016} \cite{Zeiler2014}.}
  \label{fig:DeepLearningImagePersonExample.png}
\end{figure}


\newpage
\subsection{Logistic Regression}
Logistic Regression is a machine learning algorithm which we can assume has the basic idea behind the deep learning we will explain it later. Also, Logistic Regression is one of the most used machine learning techniques for binary classification.

A simple example of logistic regression it would be if we have an algorithm for fraud detection. It takes some raw data input and detect if it is a fraud case or not let’s assume fraud case is one and a non-fraud case is zero. David Cox developed logistic regression in 1958 \cite{Cox2958}. The logistic name came from its core function logistic function which also named as \textit{Sigmoid function}  function \eqref{eq:logistic_function}. The Logistic function is shaped as S-shape. %@@@ add figure for sigma function
Also, one of these function features it can take any input real number and convert it into a value between 1 and 0.

Let's take an Example, Given x, we want to get the predictions of $\widehat{y}$ which is the estimate of $y$  when $\widehat{y}$  is presented in equation \eqref{eq:yhat_estimate}. So,to calculate the output function for logistic regression using equation \eqref{eq:logistic_regression_yhat}. Note: if we remove the Sigmoid function $\sigma$ from the equation it will be Linear Regression model and $\widehat{y}$ can be greater than 1 or negative. Figure XXXX show the Sigmoid function output. %@@@ add figure for sigmoid function
%@@@ add figure to compare linear function vs sigmoid function

\begin{equation}\label{eq:logistic_function}
  x = \frac{1}{1-e^{-x}} \quad \text{where} \quad x \in \mathbb{R}^{n_x} 
\end{equation}

\begin{equation}
  \label{eq:yhat_estimate}
    \widehat{y} = P(y=1 | x) \quad \text{where}  \quad 0 \le \widehat{y}  \le 1
  \end{equation}

\begin{equation}
  \label{eq:logistic_regression_yhat}
  \widehat{y} = \sigma(w^t x + b)  \quad \text{where:} \quad  \sigma(z) = \frac{1}{1-e^{-z}} \text{, }  w \in  \mathbb{R}^{n_x} \text{, }  b \in  \mathbb{R}  
\end{equation}


\subsubsection{Loss Error Function}

Loss Error Function is the function which describes how well our algorithm can understand  $\widehat{y}$ y b when the true label is y. It also can be defined as the difference between the true value of $y$ and the estimated value of  $\widehat{y}$. \footnote{\textit{Parts of this subsections are explained into Andrew NG Coursera courses in deep learning and It written using our understanding to this topic but the equations and the idea taken from the course  https://www.coursera.org/learn/neural-networks-deep-learning/}}. Equation \eqref  {eq:loss function} describe the loss function for Logistic Regression. There are another functions can represent the loss functions but we take the below as example. As we know $y$ is the label which should be 1 or 0. So, The reason why this function make sense to describe the loss function as below
\begin{itemize}
\item in case (y = 1) equation \eqref{eq:loss_function_log_y_1} we need $\widehat{y}$ to be big as possible to be equal or near y true which is 1. So, $ - (\log \widehat{y} )$ will get the value. Note as explained before Sigmoid function can't be greater than 1 or less than 0. %@@@ add chart here to explain
\item in case (y = 0) equation \eqref{eq:loss_function_log_y_0} we need $\widehat{y}$ to be small as possible to be equal or near y true which is 0. So, $- \log (1-\widehat{y})$  will get the value.  %@@@ add chart here to explain
  \end{itemize}
  
\begin{equation}
  \label{eq:loss function}
    \ell(y,\widehat{y}) = - (y \log \widehat{y} + (1-y) \log (1-\widehat{y}))
  \end{equation}

\begin{equation} \label{eq:loss_function_log_y_1}
\begin{split}
  \text{(if y = 1) } \quad  \ell(y,\widehat{y}) & = - (y \log \widehat{y} + (1-y) \log (1-\widehat{y})) \\
  & = - (1 \log \widehat{y} + (1-1) \log (1-\widehat{y}))\\
  & = - (\log \widehat{y} )
\end{split}
\end{equation}


\begin{equation} \label{eq:loss_function_log_y_0}
\begin{split}
  \text{(if y = 0) } \quad  \ell(y,\widehat{y}) & = - (y \log \widehat{y} + (1-y) \log (1-\widehat{y})) \\
  & = - (0 * \log \widehat{y} + (1-0) \log (1-\widehat{y}))\\
  & = - \log (1-\widehat{y})
\end{split}
\end{equation}




\subsubsection{Cost Function}
    To predict $y$ from $\widehat{y}$ we learn from the input parameters in this case it will be \textbf{\textit{(w,b)}} from Equation \eqref{eq:logistic_regression_yhat} as  \textbf{\textit{(w,b)}} is the parameters which define the relation between input dataset X and the output Y. So, Cost Function will measure how well you are doing an entire training set and the ability to understand the relation between X,Y.

Cost function \textbf{\textit{J}} in equation \eqref{eq:cost_function} is the average of loss function applied to every training example which equal the sum of the lost for each training example divided on the total number of training example.



\begin{equation}\label{eq:cost_function}
  \begin{split}
  J(w,b) & = \frac{\sum_{i=1}^{m}  \ell(y^i,\widehat{y^i})}{m} \quad \text{ where m is the total number of training example} \\
  & = \frac {- \sum_{i=1}^{m} [(y^i \log \widehat{y^i} + (1-y^i) \log (1-\widehat{y^i}))]}{m}  
  \end{split}
\end{equation}


\subsubsection{Convex Function vs Non-Convex Function }

\newpage
\subsubsection{Gradient Descent}

As we explained in the previous parts, we need to find the relation between X,Y from the input parameters \textbf{\textit{(w,b)}} which will make the cost function  \eqref{eq:cost_function} to the minimum. In other words we need to find the best value of \textbf{\textit{J(w,b)}} which will represent the relation and reduce the error between $y$ and $\widehat{y}$  So, we need to minimize \textbf{\textit{J(w,b)}}.  %@@@ add photo to represent the gradient descent

To illustrate the relation between \textbf{\textit{J(w,b)}} we will assume for simplicity the relation will be function of one variable \textbf{\textit{J(w)}}. As shown in Figure XXXX we have a curve which represent the function \textbf{\textit{J(w)}} we need to find the minimum point in this curve which is the local minimum assuming it is a  \textbf{\textit{convex function}}. We will use equation \label{eq:gredient_descent_w} to find the local minimum.

To explain how this equation works let's take a random point p from Figure XXXX let's take derivative \textit{(which by definition is the slope of the function at the point)} The slope of this function is the height (h) divided into the width (w) it is the tangent of J(w) at this point. If the derivative is positive so, w will be update minus the derivative multiplied by learning rate alpha $\alpha$ as \eqref{eq:gredient_descent_w}. We will repeat the previous step until value of w get the lowest minimum. When w get the lowest minimum the derivative will be negative so, w will start to increase again at this step the algorithm will stop.



\begin{equation}\label{eq:gredient_descent_w}
  \begin{split}
    w & := w - \alpha dw \quad \text{\textit{alpha is learning rate}}\\
      & := w - \alpha \frac{dJ(w)}{dw} \quad \text{\textit{d represent the derivative wrt w}}
  \end{split}
\end{equation}

Now, Let's generalize the above equation assume we have two parameters  \textbf{\textit{(w,b)}} and we need to calculate the cost function for  \textbf{\textit{J(w,b)}} we will work on as two steps first function \eqref{eq:gradient_descent_j_w}  wrt \textbf{\textit{(w)}} and second function \eqref{eq:gradient_descent_j_b} wrt \textbf{\textit{(b)}}

\begin{equation}\label{eq:gradient_descent_j_w}
      w := w - \alpha \frac{dJ(w,b)}{dw}
  \end{equation}

\begin{equation}\label{eq:gradient_descent_j_b}
      b := b - \alpha \frac{dJ(w,b)}{db}
  \end{equation}

\newpage
  \subsubsection{Logistic Regression derivatives}\label{logistic_bp_derivatives}

  As described we need to calculate the gradient descent to get the best $\widehat{y}$ which minimizes the total cost in equation \eqref{eq:logistic_regression_derivatives_single_example}. So, we will do backpropagation to get the value of $dz$ we need to calculate $da$ in equation \eqref{eq:logistic_regression_derivatives_da} then we will calculate $dz$ based on the output of $da$ from equation \eqref{eq:logistic_regression_derivatives_dz}. After that, We will start to take the derivative for $z$ function parameters \textbf{\textit{$w_1,w_2,b$}}. Once we got the values of \textbf{\textit{$dw_1,dw_2,db$}} we can use it to calculate the estimated values of \textbf{\textit{$w_1,w_2,b$}} in the equations \eqref{eq:logistic_regression_derivatives_dw1}, \eqref{eq:logistic_regression_derivatives_dw2}, \eqref{eq:logistic_regression_derivatives_db}

\begin{equation}\label{eq:logistic_regression_derivatives_single_example}
    \boxed{\widehat{y} = \sigma(z) = a} \longrightarrow  \boxed{ z = w^tx + b = w_1x_1+ w_2+x_2+ b} \longrightarrow \boxed{\ell(a,y)}
  \end{equation}
  \begin{equation}\label{eq:logistic_regression_derivatives_da}
      \boxed{da =  \frac{d\ell}{da} = \frac{d\ell(a,y)}{da} = - \frac{y}{a} + \frac{1-y}{1-a}}
  \end{equation}
    \begin{equation}\label{eq:logistic_regression_derivatives_dz}
    \boxed{dz = \frac{d\ell}{dz} =  \frac{d\ell(a,y)}{dz} =  \frac{d\ell}{da} .  \frac{da}{dz}} = \boxed{(- \frac{y}{a} + \frac{1-y}{1-a}) . a(a-1) } = \boxed{ a - y    }
  \end{equation}
 \begin{equation}\label{eq:logistic_regression_derivatives_dw1}
      \boxed{dw_1 = \frac{\partial\ell}{dw_1} = x_1 dz} \longrightarrow \boxed{ w_1 := w_1 - \alpha dw_1}
  \end{equation}
    \begin{equation}\label{eq:logistic_regression_derivatives_dw2}
    \boxed{dw_2 = \frac{\partial\ell}{dw_2} = x_2 dz} \longrightarrow \boxed{ w_2 := w_2  - \alpha dw_2}
  \end{equation}


  \begin{equation}\label{eq:logistic_regression_derivatives_db}
    \boxed{db = \frac{\partial\ell}{db} =  dz} \longrightarrow \boxed{ b := b - \alpha db}
\end{equation}


% \begin{align*}
% \tcbhighmath[remember as=fx]{f(x)} &= xx \text{dummy function for testing} \\
% %&= -\frac{1}{x} + \frac{1}{1}\\
% &=
% \tcbhighmath[remember,overlay={%
% \draw[blue,very thick,->] (fx.south) to[bend right] ([yshift=1mm]frame.west);}]
% {1-\frac{1}{x}.}
% \end{align*}
 \subsubsection{Implementing Logistic Regression on m example}

To implement a simple 1 iteration example below sample code simulate the program structure. First,  assume $J = 0, dw_1 = 0, dw_2 = 0,db = 0$. Then calculate the feedforward step. Then backpropagation calculate. Finally, update the parameters. We can transfer the above equation into the below python sample code. 
%%, caption=Simple Implementation for 1 iteration Logistic Regression with a Neural Network
 \begin{lstlisting}[language=Python]
   import numpy as np
   J = 0, dw_1 = 0, dw_2 = 0,db = 0, alpha = .02
   # FEED FORWARD PROPAGATION
   A = 1 / (1 + np.exp(-(np.dot(w.T,X) + b))#   Z = np.dot(w.T,X) + b
   cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))
   # BACKWARD PROPAGATION (TO FIND GRADIENT)
   dw = (1 / m) * np.dot(X, (A - Y).T) #    dz = A - Y
   db = (1 / m) * np.sum(A - Y)
   # UPDATE THE PARAMETERS
   w = w - alpha * dw
   b = b - alpha * db
 \end{lstlisting}

\newpage 
 \subsection{The Neuron}

 As we all know, Most computer research is trying to simulate the human brain as it is the most advanced smartest creation. If we are trying to check how the model understands the new information regarding for example bananas photo we can give a baby two bananas then ask him about it baby can remember it with all it new shapes. Same case if you inform any human about some information and trying to get a new inference it will automatically detect this information. So, The new research trying to simulate the human brain model into an Artificial Intelligence model to trying to get this performance. In this subsection, we will try to give an overview of the relation between the new research era and the human brain.
 
 The neuron is the foundation unit of the brain. The size of the brain is as about the size of a grain of rice. The brain contains more over 10000 neurons with average 6000 connections with other neurons\footnote{\textit{Restak, Richard M. and David Grubin. The Secret Life of the Brain. Joseph Henry Press, 2001.}}.  These massive networks allow our brain to build its knowledge about the world around us. The neuron is work by receiving the information from other neuron and process it uniquely then pass the output to other neurons this process is shown in figure \ref{fig:NeuronStructure}.

 \begin{figure}[h!] \includegraphics[width=\linewidth]{./Figures/neuron_structure.png}
  \caption{Description of neuron's structure this figure from \cite{DLFundamentals}}
  \label{fig:NeuronStructure}
\end{figure}

 
 How do we learn a new concept? \textit{The neuron receives its input from dendrites. The incoming neuron connection is dynamically strengthened or weakened based on how often it is used, and the strength of each connection determines the contribution of the input to the neuron's output. Based on the connection strength it will have weight then the input is summed in the cell body. This sum is transformed into a new signal which is propagated along the cell's axon and sent to other neurons\cite{DLFundamentals}}.

 The above biological model can be translated into an Artificial Neural Network as described in figure XXXX. %@@@ add figure for neuron input format here
 We have an input $x_1,x_2,x_3,...,x_n$ every input has its own strength (weight) $w_1,w_2,w_3,...,w_n$. We Sum the multiplication of X and W to get the logit of the neuron, $z =  \sum_{i=0}^{n} x_i w_i $. The logit is passed throw a function $f$ to produce the output $ y = f(z)$ the output will be the input to other neurons. Note: In many cases, the logit can also include a bias constant. So, in this case the function will be $$ y = f(\sum_{i=0}^{n} x_i w_i + b)$$
 
 \subsection{The Neural Network Representation}
 As explained previously, We have been trying to simulate the human brain model into our research work in Deep Neural Network. So, We will have multi-layers to allows the model to get in-depth knowledge and more computation performance to simulate the human brain.

 Now, we will represent the functions per layer as below equations where \textit{l is refer to layer number, i refer to the node number in the layer}\eqref{eq:nn_multi_layer}
 
\begin{equation}\label{eq:nn_multi_layer}
  \boxed{z^l =  W^l x + b^l} \longrightarrow \boxed{a_i^l =  \sigma(z^l)} \longrightarrow \boxed{\ell(a^l,y)}
\end{equation}

What is the Neural Networks component?

\textbf{Input Layer:} Input layers is the input data raw for the network it is denoted as $a^0$.
\textbf{Hidden Layers:} The layers between the input layers and the output layer it can be any number of layers. It also has a set of weighted input and produces an output through an activation function. Every layer in the hidden layer transmits the output to the other hidden layer as an input feature figure XXXX shows this relations. %@@@ NN representation

\textbf{Output Layer:} It is one output layer with have the final results from the hidden layers.

 \subsection{Neural Network Computation}
 In this subsection, We will show as example on how we can compute the Neural Networks for every layer. In figure XXXX we have example of one layer we will continue explain on this example\eqref{eq_nn_one_layer}.

 \begin{subequations}\label{eq_nn_one_layer}
   \begin{align}
     Z_1^{[1]} & = w_1^{[1]T} x + b_1^{[1]} , a_1^{[1]} = \sigma(Z_1^{[1]}) \\
     Z_2^{[1]} & = w_2^{[1]T} x + b_2^{[1]} , a_2^{[1]} = \sigma(Z_2^{[1]})\\
     Z_3^{[1]} & = w_3^{[1]T} x + b_3^{[1]} , a_3^{[1]} = \sigma(Z_3^{[1]})\\
     Z_4^{[1]} & = w_4^{[1]T} x + b_4^{[1]} , a_4^{[1]} = \sigma(Z_4^{[1]})
 \end{align}
\end{subequations}
If we need to compute the above equations it will be simply be represented as vectorized way below matrix shows how we can implement it.
\[
z^{[1]} = 
\left[
  \begin{array}{ccc}
     w^{[1]T}_{1} \\
     w^{[1]T}_{2} \\
     w^{[1]T}_{3} \\
     w^{[1]T}_{4} \\  
  \end{array}
\right]\cdot
\left[
  \begin{array}{c}
           x_{1} \\
           x_{2} \\
    x_{3} %\vdots \\
  \end{array}
\right] +
\left[
  \begin{array}{c}
           b_{1}^{[1]}\\
           b_{2}^{[1]}\\
           b_{3}^{[1]}\\
           b_{4}^{[1]}
  \end{array}
\right] =
\left[
  \begin{array}{cccc}
     w^{[1]T}_{1} x + b _1^{[1]}\\
     w^{[1]T}_{2} x + b _2^{[1] }\\
     w^{[1]T}_{3} x + b _3^{[1] }\\
     w^{[1]T}_{4} x + b _4^{[1] }\\  
  \end{array}
\right] =
\left[
  \begin{array}{c}
    z_1^{[1]} \\
    z_2^{[1]} \\
    z_3^{[1]} \\
    z_3^{[1]}
  \end{array}
\right]
\]

\newpage
\subsubsection{Linear Neurons and Their Limitations}

Now, We explained the equations for the feedforward Neural Network. We have only one point we need to discuss it which is the Activation function. Let's assume we will continue use linear function $y= w x + b$. So, if we have mutli-layer networks for example equation \eqref{eq_linear_fun_limitations} it will end as linear function because composition of two linear function will be linear function. So, we will not compute deep computation and we will get limited information from the networks. So, to be able to detect the deep information we will use different function for the hidden layers example: Tanh\eqref{eq:nn_tanh}, Sigmoid\eqref{eq:logistic_function} and Relu\eqref{eq:nn_relu}. Most of binary classification problems use Sigmoid function for output layer. Also, we can use the same functions for the output but we can also use the linear for activation function in some cases. 

\begin{subequations}\label{eq_linear_fun_limitations}
   \begin{align}
     Z^{[1]} & = w_1^{[1]T} x + b_1^{[1]} , a_1^{[1]} = \sigma(Z_1^{[1]}) \\
     Z^{[2]} & = w^{[2]T} a^1 + b^{[2]} = w^{[2]T} (w^{[1]T}x + b^{[1]}) + b^{[2]}\\
             & = (w^{[1]T}W^{[2]T})x + (w^{[2]}b^{[1]}+ b^{[2]})\\
             & = W' x + b'
\end{align}
\end{subequations}



 
\begin{equation}\label{eq:nn_tanh}
  a = tanh(z) =\frac{e^z-e^{-z}}{e^z+e^{-z}}
\end{equation}%@@@ add figure for tanh



\begin{equation}\label{eq:nn_relu}
  a = max (0,z)
\end{equation}%@@@ add figure for relu

\subsubsection{Softmax Output Layers}
Sometimes our problem has multi-output results not only 1 or 0. For example, we have a problem to recognize the characters from 0 to 9 in MNIST dataset, But we will not be able to recognize digits with 100\% confidence. So, we will use the probability distribution to give us a better idea of how confident we are in our predictions. The result will be an output vector of the form of the $\sum_{i = 0}^9P_i=1$

This is achieved by using a special output layer named softmax layer. This layer is differ from the other as the output of a neuron in a softmax layer is depending on the output of all the other neurons in its layer. This because its sum of all output equal 1. If we assume $z_i$ be the logit of $i^{th}$ softmax neuron, we can normalize by setting its output to represented from eq \eqref{eq:nn_softmax_fun}:

\begin{equation}\label{eq:nn_softmax_fun}
  y_i=\frac{e^{z_i}}{\sum_je^{z_j}}
\end{equation}

The strong prediction will have a value entry in the vector close to 1, while the other entries will be close to 0. The weak prediction will have multiple possible labels has almost the equal values\cite{DLFundamentals}.



\subsubsection{Forward-Propagation in a Neural Networks}
%@@@ add figure to show the network example similar https://www.coursera.org/learn/neural-networks-deep-learning/lecture/MijzH/forward-propagation-in-a-deep-network
We will take the below figure XXXX as example of Deep Neural Network. So, to calculate the Forward propagation we will follow the below equation \eqref{eq_feedfarward_DL}. Note: we assume $X = a^{[0]}$ as initial function notation. Also, $\widehat{Y}= g(Z^{[4]}=A^{[4]}$ as the final output layer.
\begin{equation}\label{eq_feedfarward_DL}
     Z^{[l]}  = w^{[l]} a^{l-1} + b^{[l]} , A^{[1]} = g^{l}(Z^{[l]})
   \end{equation}
   % @@@ figure to show the input forward and back propagation steps



\subsubsection{Back-Propagation in a Neural Networks}

We explained previously,  how neural networks could learn their weights using gradient descent algorithm. In this part, we will explain how to compute the gradient of the cost function.

To compute the gradient descent in Neural Networks, we use an algorithm named \textit{backpropagation}. The backpropagation algorithm was initially invented in the 1970s, but it wasn't shining until one of the most important papers in this field published in 1986 %@@@cite https://www.nature.com/articles/323533a0
which describes several neural networks where backpropagation has a significant performance better than the earlier approaches and making it possible to use neural networks to solve problems which were previously not possible to be solved. Now, the backpropagation is the backbone for the learning in neural networks.%@@@cite Michael A. Nielsen, "Neural Networks and Deep Learning", Determination Press, 2015

The backpropagation not only an algorithm which gives us the expression for partial derivative of the cost function $C$ with respect to wights $w$ and bias $b$ but also it gives is an intuations about the change of the cost function while changing its variables $w \& b$ and its effect to the overall network.

As explained in logistic regression section (\ref{logistic_bp_derivatives}) how we can calculate the derivatives for logistic regression with one layer using this equations\eqref{eq:logistic_regression_derivatives_single_example},\eqref{eq:logistic_regression_derivatives_da},\eqref{eq:logistic_regression_derivatives_dz},\\
\eqref{eq:logistic_regression_derivatives_dw1},\eqref{eq:logistic_regression_derivatives_dw2},\eqref{eq:logistic_regression_derivatives_db}.\\
We will generalize the derivatives equations to be for $l$ layers from the below equations\eqref{eq_nn_bp_l_layers}.
 \begin{subequations}\label{eq_nn_bp_l_layers}
   \begin{align}
     dz^{[l]} & = da^{[l]} * g^{[l]'}(z^{l}) \\
     dw^{[l]} & = dz^{[l]} \cdot a^{[l-1]} \\
     db^{[l]} & = dz^{[l]} \\
     da^{[l-1]} & = W^{[l]T} \cdot dz^{[l]} %\\
%     dz^{[l]} & =  (W^{[l+1]T} \cdot dz^{[l+1]}) * g^{[l]'}(z^{l}) \quad \text{from 2.24d in 2.24a}
 \end{align}
\end{subequations}
We can vectorize the above equation for Neural Network implementation as below equations\eqref{eq_nn_bp_l_layers_vectorize}.
 \begin{subequations}\label{eq_nn_bp_l_layers_vectorize}
   \begin{align}
     dz^{[l]} & = dA^{[l]} * g^{[l]'}(z^{l}) \\
     dw^{[l]} & = \frac{1}{m} dz^{[l]} \cdot A^{[l-1]T} \\
     db^{[l]} & = \frac{1}{m} \text{ np.sum(}dz^{[l]}\text{,axis=1,keepdims = true)} \\
     dA^{[l-1]} & = W^{[l]T} \cdot dz^{[l]} %\\
%     dz^{[l]} & =  (W^{[l+1]T} \cdot dz^{[l+1]}) * g^{[l]'}(z^{l}) \quad \text{from 2.24d in 2.24a}
 \end{align}
\end{subequations}

If we checked the input variable in the backpropagation we will find it is $da^{l}$ and this is the derivative of \eqref{eq:loss function} which we can get it as explained previously from \eqref{eq:logistic_regression_derivatives_da} this is the formula for final layer in the feedforward step. If we need to calculate the vectorize version of this equation we can use equation\eqref{eq:logistic_regression_derivatives_da_vectorize}

 \begin{equation}\label{eq:logistic_regression_derivatives_da_vectorize}
      da =  \frac{d\ell}{da} = \frac{d\ell(a,y)}{da} = (- \frac{y^{[1]}}{a^{[1]}} + \frac{1-y^{[1]}}{1-a^{[1]}} \ldots - \frac{y^{[m]}}{a^{[m]}} + \frac{1-y^{[m]}}{1-a^{[m]}} )
  \end{equation}
   
\newpage
\subsubsection{How we Initialize the Wights}

    As we explained previously in Logistic regression, We initialized the weights to Zero. However, in Deep Neural Networks it will not work. Note: It is okay to initialize the Bias to Zero but the wights it will not works. Let's see what will happen if we initialize the weights and Bias to Zero.
 
  % @@@https://www.coursera.org/learn/neural-networks-deep-learning/lecture/XtFPI/random-initialization figure
  Assume from figure XXXX we have two input vectors $x_1,X_2$ if we initialize $W^{[1]}$ to Zero from equation\eqref{eq:nn_weights_init_zero} and $b^{[1]}$ to Zeros. So, $a_1^{[1]}=a_2^{[1]}$ because both of the hidden units compute the same functions. Also, $W^{[2]}=[0 0]$ Then when we will compute the backpropagation we will find that $dz_1^{[1]}=dz_2^{[2]}$. So, After every iteration, we will find that the two hidden units calculate the same function and we will not get more information from this Deep Neural Network. We need to highlight that the main idea from Neural Networks as explained before is every hidden unit should work to get a new piece of information. The more hidden unit, the more hidden information we will get but if we initialize it to Zero. It will be the same function which is calculated, and we will not get any new information\footnote{\textit{Parts of this subsection are explained into Andrew NG Coursera course in deep learning and It written using our understanding to this topic but the equations and the idea taken from the course  https://www.coursera.org/learn/neural-networks-deep-learning/}}.

\begin{subequations}\label{eq:nn_weights_init_zero}
\begin{align}
  W^{[1]} = \begin{bmatrix} 0 & 0\\ 0 & 0 \end{bmatrix} \quad b^{[1]} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \\
  W^{[2]} = \begin{bmatrix} 0 & 0 \end{bmatrix} \\
  a_1^{[1]} = a_2^{[1]} \quad     dz_1^{[1]} = dz_2^{[1]}\\
  dw = \begin{bmatrix} u & u \\ v & v \end{bmatrix} \quad W^{[1]} = W^{[1]} - \alpha dw
\end{align}
\end{subequations}
     
To initialize weights and to get the maximum value of the neural network computation we should initialize the weight by any small random numbers to avoid the big weights which will tend to get the small slope from the Z where $Z^{[1]}= W^{[1]} X + b^{[1]}$ For example, if we use tanh we will get the big tail values $a^{[1]}= g^{[1]}(Z^{[1]})$. So, the big weights we more likely to get slow learning rate. 
  
%@@@$\subsubsection{Deep Learning Running Cycle}
%@@@@ Cost function vs Loss Function vs Gradient Desend 

\newpage
\subsection{Recurrent Neural Networks (RNNs)}

Deep Neural Networks shows its ability to solve many problems. However, in some use cases, Naive Neural Network architecture cannot works or get the expected results. One of the famous example related to this issue in the NLP tasks when working on a text problem for example, If we say our Harry is the king and Elizabeth is the queen, and we need our model to understand from the sentence that, Harry is he and Elizabeth is she. Also, if this word appears again, we need the model to detect that Harry is a person.

This type of problem has a dependency on the input text and how to get the output prediction based on the provided information from the input.

As explained previously, Most of the research in this area trying to simulate human brains. So, we will not find anyone every time trying to think about something start from scratch it always starts from another related point. Example, What is the human do if he tries to connect the information to generate the knowledge about something.

RNN shows its ability to work on sequence data and its related application problems such as natural language\cite{Mikolov_et_al}. showed the effective of RNN on language modeling. There are many problems which based on this idea of dependency. For example,
\begin{itemize}
\item Time series anomaly detection.
\item Speech recognition.
\item Music Composition.
\item Image captioning.
\item Stock market prediction.
\item Translation.
\end{itemize}
So, What are the problems in the Naive Neural Network architecture?
\begin{itemize}
\item Input and output length can be the different length in a different example.
\item The most important issue is that the Naive architecture cannot share features learned across different positions of text. In this case, we will lose the learned feature, and the lake of dependency, in this case, will affect the overall performance.
\end{itemize}
What is the new proposed architecture which can provide a way to share the features between the Network?
\begin{itemize}
\item First, Assume we have input features $x_1, x_2, x_n$ in the old architecture we input all these features to the Neural Network but now we will input for example $x_1$ and take the output activation from $a^{<1>}$ to be a feature input with $x_2$ then take the output activation from $a^{<2>}$ as input to $x_3$ similar till $x_n$ figures \ref{fig:RNN-rolled-loop.png}, \ref{fig:andew_ng_feedfarward} shows an example. So, This new change will allow us to share the learned feature between the networks input data. Also, we can thing about it as multiple copies of the same network, each passing a message to a successor\cite{colah}.

   \begin{figure}[h!] \includegraphics[width=\linewidth]{./Figures/RNN-unrolled.png}
  \caption{Recurrent Neural Networks Loops\cite{colah}}
  \label{fig:RNN-rolled-loop.png}
\end{figure}

\begin{figure}[h!] \includegraphics[width=\linewidth]{./Figures/andew_ng_feedfarward.png}
  \caption{Recurrent Neural Networks feedfarward \textit{ This figure from Andrew NG course sequence models https://www.coursera.org/learn/nlp-sequence-models/ }}
  \label{fig:andew_ng_feedfarward}
\end{figure}
% @@@ to be replaced by new owned ones

% @@@add example about the new architecture

\item Second, The feedfarward will be compute for time t and then we will calculate the loss at step t. The final loss is the sum of loss at every step t eq\eqref{eq:rnn_feedfarward} explains the steps for feedfarward. Note: The backpropagation here will be calculated though time at every step.
%@@@ change the notations from Andrew to Blog
  \begin{subequations}\label{eq:rnn_feedfarward}
\begin{align}
  a^{<t>} & = g(W_{aa}a^{<t-1>}+ W_{ax}x^{<t>}+b_a)\\
   & = g(W_a[a^{<t-1>},x^{<t>}]+ b_a)\\
  \widehat{y}^{<t>} & = g(W_{ya}a^{<t>}+ b_y)
  \\ \ell^{<t>}(\widehat{y}^{<t>},y^{<t>}) & = - (y^{<t>} \log \widehat{y}^{<t>} + (1-y^{<t>}) \log (1-\widehat{y}^{<t>}))
\\ \ell(\widehat{y},y) & = \sim_{t=1}^{T_m} \ell^{<t>}(\widehat{y}^{<t>},y^{<t>})                                              
\end{align}
\end{subequations}

  
 \end{itemize}


 \subsubsection{Vanishing Gradient with RNNs}
 
 As we explained, RNN works on sequential data, and the idea is to predict new output not only based on the input data vector but also, other input vectors. Due to the recurrent structure in RNNs, it tends to suffer from long-term dependency to simplify this point let’s have an example, the following sentence \\
 \textit{Waleed Yousef who is Associate Professor at Helwan University and teaching Data Science courses and its dependencies \textbf{\underline{was}} got Ph.D. in Computer Engineering from GWU at 2006.}.

 In the previous example, to predict the word was is depending on long dependency to check if Waleed is singular or not to be consistent. Also, shows how some problems need the long-term dependencies handling.[Bengio et al.,1994]\cite{Bengio_ et_ al} showed that Basic RNNs has a problem in long-term dependency.  Another problem which may happen into basic Neural Networks is gradient exploding. One of the side-effects of gradient exploding is exponentially large gradient which causes our parameters to be so large. So, the Neural Networks parameters will have a server problem. Another fetal problem with Basic Neural Networks is overfitting problems [Zaremba et al., 2014]\cite{Zaremba_et_al}.
 
 So, to solve this learning problem [Hochreiter and Schmidhuber, 1997] introduced Long Short-Term Memory which helps to reduce the dependency problem using memory cell and forget gate.
\newpage
\subsection{Long Short Term Memory networks (LSTMs)}


Long Short Term Memory networks – aka “LSTMs” – are a special type of RNN, capable of learning long-term dependencies. To solve the vanishing gradient problem for long-term dependencies, [Hochreiter and Schmidhuber, 1997]\cite{Hochreiter} suggested new cell architecture for RNN by adding Long Short Term Memory which significantly reduced the long-term dependency problem using memory cell and forget gate.

 LSTMs designed to help solving the long-term dependency problem and to hold information in memory for long periods of time. It also, use same RNNs sequential model but with adding some gating mechanism structure to every cell.

 Both Basic RNNs and LSTM have the form of a chain of repeating modules of neural network. The main difference is the structure of the Networks.
 
\begin{figure}[h!] \includegraphics[width=\linewidth]{./Figures/LSTM-SimpleRNN.png}
  \caption{The repeating module in a standard RNN contains a single layer.\cite{colah}}
  \label{fig:LSTM-SimpleRNN}
\end{figure}

 In Basic RNNs it is very simple structure for every layer with simple output function \ref{fig:LSTM-SimpleRNN}. But in LSTMs it has four interacting layers \ref{fig:LSTM-cell-chaining}.


\begin{figure}[h!] \includegraphics[width=\linewidth]{./Figures/LSTM-cell-chaining.png}
  \caption{The repeating module in an LSTM contains four interacting layers.\cite{colah}}
  \label{fig:LSTM-cell-chaining}
\end{figure}


\subsubsection{LSTM Gate Mechanism}

The main component of LSTM is the cell state; It allows the information to pass through along it unchanged. In figure xxx the top line show the information flow through the cell. The LSTM cell can add or remove information to the cell state using the Gating mechanism. 

Gates's idea is a methodology to manage the way how and which information pass or not. It controls information flow through the cell. It has three of these gates. They are consist of a sigmoid neural network layer \ref{fig:LSTM-Cell-state} and a pointwise multiplication operation \ref{fig:LSTM-gate}.

Sigmoid function output values between zero and one. If the value is one these means that everything should pass, while if the value is zero these means do not pass anything. So, the value output from the sigmoid function refers to the amount of each component should be passed.

\begin{figure}[h!]
    \centering
        \includegraphics[width=\textwidth]{./Figures/LSTM-Cell-state.png}
        \caption{LSTM top horizontal line working as the medium for information flow}
        \label{fig:LSTM-Cell-state}
\end{figure}
\begin{figure}[h!]
    \centering
        \includegraphics[width=0.2\textwidth]{./Figures/LSTM-gate.png}
        \caption{Cell gate with sigmoid function and a pointwise multiplication operation}
        \label{fig:LSTM-gate}
\end{figure}


\newpage
\subsubsection{Gated Recurrent Units (GRUs)}

 In RNN Gated recurrent units (GRUs) are a gating mechanism, introduced in 2014 by Kyunghyun Cho et al. \cite{Cho_et_al}. It works to overcome the problem for long-term dependencies. It also aimd to solve the vanishing gradient problem from Basic RNNs.It proposed a new architecture in RNN cell units.

 

 


 % @@@ example about the weakness about RNN and tiddy example when we talk about Bi-LSTM 
